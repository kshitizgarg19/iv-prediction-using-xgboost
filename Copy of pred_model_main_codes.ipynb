{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":561},"executionInfo":{"elapsed":4427,"status":"error","timestamp":1752040062741,"user":{"displayName":"kshitiz Garg","userId":"07822256762517209730"},"user_tz":-330},"id":"z1q2S0es4baQ","outputId":"59058781-9a55-44bc-a6dd-afabf012cf87"},"outputs":[{"output_type":"error","ename":"ParserError","evalue":"Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1-695017654.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# ================== LOAD & PARSE DATA ==================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."]}],"source":["# feature making\n","\n","\n","import pandas as pd\n","import numpy as np\n","\n","# ================== CONFIGURATION ==================\n","main_file = \"/content/drive/MyDrive/colab/Copy of nifty_spot_fut_data.csv\"\n","output_file = \"/content/drive/MyDrive/colab/nifty_all_features_with_targets_new.csv\"\n","\n","# ================== LOAD & PARSE DATA ==================\n","df = pd.read_csv(main_file)\n","df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n","df['time'] = pd.to_datetime(df['time'], format='%H:%M:%S').dt.time\n","df = df.sort_values(['date', 'time'])\n","\n","# ================== FEATURE ENGINEERING ==================\n","df['atm_iv'] = df['atm_c_iv'] + df['atm_p_iv']\n","grouped = df.groupby('date', group_keys=False)\n","\n","# Current day high/low and their differences\n","df['curr_day_high'] = grouped['spot_ltp'].cummax()\n","df['curr_day_low'] = grouped['spot_ltp'].cummin()\n","df['curr_day_high_diff'] = df['curr_day_high'] - df['spot_ltp']\n","df['curr_day_low_diff'] = df['spot_ltp'] - df['curr_day_low']\n","\n","# Rolling day-based high/low features\n","daily_agg = df.groupby('date')['spot_ltp'].agg(['min', 'max']).rename(\n","    columns={'min': 'daily_low', 'max': 'daily_high'})\n","daily_agg['5d_high'] = daily_agg['daily_high'].shift(1).rolling(5, min_periods=1).max()\n","daily_agg['5d_low'] = daily_agg['daily_low'].shift(1).rolling(5, min_periods=1).min()\n","daily_agg['3d_high'] = daily_agg['daily_high'].shift(1).rolling(3, min_periods=1).max()\n","daily_agg['3d_low'] = daily_agg['daily_low'].shift(1).rolling(3, min_periods=1).min()\n","daily_agg['1d_high'] = daily_agg['daily_high'].shift(1)\n","daily_agg['1d_low'] = daily_agg['daily_low'].shift(1)\n","\n","df = df.merge(daily_agg[['5d_high','5d_low','3d_high','3d_low','1d_high','1d_low']],\n","              left_on='date', right_index=True, how='left')\n","\n","df['diff_5d_high'] = df['spot_ltp'] - df['5d_high']\n","df['diff_5d_low'] = df['spot_ltp'] - df['5d_low']\n","df['diff_3d_high'] = df['spot_ltp'] - df['3d_high']\n","df['diff_3d_low'] = df['spot_ltp'] - df['3d_low']\n","df['diff_1d_high'] = df['spot_ltp'] - df['1d_high']\n","df['diff_1d_low'] = df['spot_ltp'] - df['1d_low']\n","\n","# Rolling window high/low and related features\n","windows = [60, 180, 300,600,1500,3600]\n","for w in windows:\n","    df[f'high_{w}s'] = grouped['spot_ltp'].transform(lambda x: x.rolling(w, min_periods=1).max())\n","    df[f'low_{w}s'] = grouped['spot_ltp'].transform(lambda x: x.rolling(w, min_periods=1).min())\n","    df[f'diff_high_{w}s'] = df['spot_ltp'] - df[f'high_{w}s']\n","    df[f'diff_low_{w}s'] = df['spot_ltp'] - df[f'low_{w}s']\n","\n","# Volatility, returns, stability for spot, future, and IV\n","for w in windows:\n","    df[f'spot_ret{w}'] = grouped['spot_ltp'].transform(lambda x: x.pct_change(w, fill_method=None))\n","    df[f'spot_rv{w}'] = grouped['spot_ltp'].transform(lambda x: x.rolling(w).std())\n","    df[f'spot_stability_{w}'] = grouped['spot_ltp'].transform(lambda x: x.rolling(w).mean() / x.rolling(w).std())\n","    df[f'fut_ret{w}'] = grouped['fut_1_ltp'].transform(lambda x: x.pct_change(w, fill_method=None))\n","    df[f'fut_rv{w}'] = grouped['fut_1_ltp'].transform(lambda x: x.rolling(w).std())\n","    df[f'fut_stability_{w}'] = grouped['fut_1_ltp'].transform(lambda x: x.rolling(w).mean() / x.rolling(w).std())\n","    df[f'iv_ret{w}'] = grouped['atm_iv'].transform(lambda x: x.pct_change(w, fill_method=None))\n","    df[f'iv_rv{w}'] = grouped['atm_iv'].transform(lambda x: x.rolling(w).std())\n","    df[f'iv_stability_{w}'] = grouped['atm_iv'].transform(lambda x: x.rolling(w).mean() / x.rolling(w).std())\n","    df[f'fut_vol_chg{w}'] = grouped['fut_1_vol'].transform(lambda x: x.diff(w))\n","\n","# Volume/OI, spread, skew\n","df['voi_ratio'] = df['fut_1_vol'] / df['fut_1_oi']\n","df['basis_spread'] = df['fut_1_ltp'] - df['spot_ltp']\n","df['iv_skew'] = df['atm_c_iv'] - df['atm_p_iv']\n","\n","# Gap features\n","prev_close = df.groupby('date')['spot_ltp'].last().shift(1).rename('prev_day_close')\n","curr_open = df.groupby('date')['spot_ltp'].first().rename('curr_day_open')\n","gap_features = pd.concat([prev_close, curr_open], axis=1)\n","gap_features['gap'] = gap_features['curr_day_open'] - gap_features['prev_day_close']\n","gap_features['gap_direction'] = gap_features['gap'].apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n","gap_features['gap_pct'] = gap_features['gap'] / gap_features['prev_day_close']\n","df = df.merge(gap_features, left_on='date', right_index=True, how='left')\n","\n","# ================== TARGET FEATURES ==================\n","# Compute spot returns\n","df['spot_ret'] = df.groupby('date')['spot_ltp'].transform(lambda x: x.pct_change(fill_method=None))\n","\n","# Re-group after spot_ret addition\n","grouped = df.groupby('date', group_keys=False)\n","\n","# Calculate targets\n","for w in [60, 180, 300,600,1500,3600]:\n","    df[f'realized_vol_{w}'] = df.groupby('date')['spot_ret'].transform(lambda x: x.rolling(w, min_periods=1).std())\n","    df[f'realized_vol_target_{w}'] = (df[f'realized_vol_{w}'].shift(-w) > df[f'realized_vol_{w}']).astype(int)\n","    df[f'spot_dir_target_{w}'] = df.groupby('date')['spot_ltp'].transform(lambda x: (x.shift(-w) > x).astype(int))\n","    df[f'iv_target_{w}'] = df.groupby('date')['atm_iv'].transform(lambda x: (x.shift(-w) > x).astype(int))\n","\n","# ================== FINAL CLEANUP & SAVE ==================\n","df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","df.dropna(inplace=True)\n","df.to_csv(output_file, index=False)\n","print(f\"âœ… All features and targets computed and saved to '{output_file}'\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4zc-h376mVoO"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PwCVS3Pb4fiV"},"outputs":[],"source":["import pandas as pd\n","\n","# Path to your original file\n","file_path = '/content/drive/MyDrive/colab/nifty_all_features_with_targets_new.csv'\n","\n","# List of columns to keep (as you provided)\n","columns_to_keep = [\n","    'date', 'time', 'dte',\n","    'curr_day_high_diff', 'curr_day_low_diff',\n","    'diff_5d_high', 'diff_5d_low', 'diff_3d_high', 'diff_3d_low', 'diff_1d_high', 'diff_1d_low',\n","    'diff_high_60s', 'diff_low_60s', 'diff_high_180s', 'diff_low_180s',\n","    'diff_high_300s', 'diff_low_300s',\n","    'spot_ret60', 'spot_rv60', 'spot_stability_60', 'fut_ret60', 'fut_rv60', 'fut_stability_60',\n","    'iv_ret60', 'iv_rv60', 'iv_stability_60', 'fut_vol_chg60',\n","    'spot_ret180', 'spot_rv180', 'spot_stability_180', 'fut_ret180', 'fut_rv180', 'fut_stability_180',\n","    'iv_ret180', 'iv_rv180', 'iv_stability_180', 'fut_vol_chg180',\n","    'spot_ret300', 'spot_rv300', 'spot_stability_300', 'fut_ret300', 'fut_rv300', 'fut_stability_300',\n","    'iv_ret300', 'iv_rv300', 'iv_stability_300', 'fut_vol_chg300',\n","    'spot_ret600', 'spot_rv600', 'spot_stability_600', 'fut_ret600', 'fut_rv600', 'fut_stability_600',\n","    'iv_ret600', 'iv_rv600', 'iv_stability_600', 'fut_vol_chg600',\n","    'spot_ret1500', 'spot_rv1500', 'spot_stability_1500', 'fut_ret1500', 'fut_rv1500', 'fut_stability_1500',\n","    'iv_ret1500', 'iv_rv1500', 'iv_stability_1500', 'fut_vol_chg1500',\n","\n","    'spot_ret3600', 'spot_rv3600', 'spot_stability_3600', 'fut_ret3600', 'fut_rv3600', 'fut_stability_3600',\n","    'iv_ret3600', 'iv_rv3600', 'iv_stability_3600', 'fut_vol_chg3600',\n","\n","    'voi_ratio', 'basis_spread', 'iv_skew',\n","    'prev_day_close', 'curr_day_open', 'gap', 'gap_direction', 'gap_pct',\n","    'spot_ret',\n","    'realized_vol_60', 'realized_vol_target_60', 'spot_dir_target_60', 'iv_target_60',\n","    'realized_vol_180', 'realized_vol_target_180', 'spot_dir_target_180', 'iv_target_180',\n","    'realized_vol_300', 'realized_vol_target_300', 'spot_dir_target_300', 'iv_target_300',\n","     'realized_vol_600', 'realized_vol_target_600', 'spot_dir_target_600', 'iv_target_600',\n","     'realized_vol_1500', 'realized_vol_target_1500', 'spot_dir_target_1500', 'iv_target_1500',\n","     'realized_vol_3600', 'realized_vol_target_3600', 'spot_dir_target_3600', 'iv_target_3600'\n","]\n","\n","# Load the CSV\n","df = pd.read_csv(file_path)\n","\n","# Filter the DataFrame to only the columns you want (ignore missing columns)\n","filtered_df = df[[col for col in columns_to_keep if col in df.columns]]\n","\n","# Save to a new CSV\n","output_path = '/content/drive/MyDrive/colab/nifty_filtered_features_new.csv'\n","filtered_df.to_csv(output_path, index=False)\n","\n","print(f\"Filtered CSV saved to: {output_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSr0y9zJ4iM4"},"outputs":[],"source":["# 3 month training and 2 month testing (choose any 10  random samples in one trial)\n","\n","import pandas as pd\n","import numpy as np\n","import itertools\n","import xgboost as xgb\n","import random\n","import time\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# ========== CONFIGURATION ==========\n","DATA_PATH ='/content/drive/MyDrive/Copy of nifty_filtered_features.csv'\n","OUTPUT_CSV = \"//content/drive/MyDrive/Colab Notebooks/rv60_180_300_1d_datedsplit.csv\"\n","\n","day_groups = {\n","     '5d': ['diff_5d_high', 'diff_5d_low'],\n","     '3d': ['diff_3d_high', 'diff_3d_low'],\n","    '1d': ['diff_1d_high', 'diff_1d_low'],\n","}\n","\n","window_groups = {\n","    '60': [\n","        'diff_high_60s', 'diff_low_60s', 'spot_ret60', 'spot_rv60', 'spot_stability_60',\n","        'fut_ret60', 'fut_rv60', 'fut_stability_60', 'iv_ret60', 'iv_rv60', 'iv_stability_60', 'fut_vol_chg60'\n","    ],\n","    '180': [\n","        'diff_high_180s', 'diff_low_180s', 'spot_ret180', 'spot_rv180', 'spot_stability_180',\n","        'fut_ret180', 'fut_rv180', 'fut_stability_180', 'iv_ret180', 'iv_rv180', 'iv_stability_180', 'fut_vol_chg180'\n","    ],\n","    '300': [\n","        'diff_high_300s', 'diff_low_300s', 'spot_ret300', 'spot_rv300', 'spot_stability_300',\n","        'fut_ret300', 'fut_rv300', 'fut_stability_300', 'iv_ret300', 'iv_rv300', 'iv_stability_300', 'fut_vol_chg300'\n","    ]\n","}\n","\n","other_features = [\n","    'curr_day_high_diff', 'curr_day_low_diff', 'voi_ratio', 'basis_spread', 'iv_skew',\n","    'prev_day_close', 'curr_day_open', 'gap', 'gap_direction', 'gap_pct', 'spot_ret'\n","]\n","\n","target_features = [\n","    'iv_target_60','iv_target_180', 'iv_target_300',\n","     'realized_vol_target_60', 'realized_vol_target_180', 'realized_vol_target_300',\n","\n","    'spot_dir_target_60', 'spot_dir_target_180', 'spot_dir_target_300']\n","\n","# --- Hyperparameter Grid ---\n","MAX_DEPTHS = [2, 4]\n","SUBSAMPLES = [0.7]\n","COLSAMPLE_BYTREE = [0.7]\n","MIN_CHILD_WEIGHT = [1, 3]\n","GAMMA = [0.1]\n","REG_ALPHA = [0.01]\n","REG_LAMBDA = [0.1]\n","COLSAMPLE_BYLEVEL = [0.6]\n","COLSAMPLE_BYNODE = [0.6]\n","MAX_DELTA_STEP = [1]\n","\n","HYPERPARAM_GRID = [\n","    {\n","        'max_depth': md,\n","        'subsample': ss,\n","        'colsample_bytree': cbt,\n","        'min_child_weight': mcw,\n","        'gamma': gm,\n","        'reg_alpha': ra,\n","        'reg_lambda': rl,\n","        'colsample_bylevel': cbl,\n","        'colsample_bynode': cbn,\n","        'max_delta_step': mds,\n","        'booster': 'gbtree',\n","        'tree_method': 'gpu_hist',\n","        'predictor': 'gpu_predictor',\n","        'device': 'cuda',\n","        'nthread': 6,\n","        'verbosity': 2,\n","        'validate_parameters': False\n","    }\n","    for md in MAX_DEPTHS\n","    for ss in SUBSAMPLES\n","    for cbt in COLSAMPLE_BYTREE\n","    for mcw in MIN_CHILD_WEIGHT\n","    for gm in GAMMA\n","    for ra in REG_ALPHA\n","    for rl in REG_LAMBDA\n","    for cbl in COLSAMPLE_BYLEVEL\n","    for cbn in COLSAMPLE_BYNODE\n","    for mds in MAX_DELTA_STEP\n","]\n","\n","def create_xgb_model(params):\n","    return xgb.XGBClassifier(\n","        **params\n","    )\n","\n","def run_modeling():\n","    df = pd.read_csv(DATA_PATH)\n","    df['date'] = pd.to_datetime(df['date'])\n","\n","    window_keys = list(window_groups.keys())\n","    window_combos = []\n","    for r in range(1, len(window_keys)+1):\n","        window_combos += list(itertools.combinations(window_keys, r))\n","\n","    results = []\n","\n","    for day_label, day_features in day_groups.items():\n","        print(f\"\\n=== Processing Day Group: {day_label} ===\")\n","\n","        for window_combo in window_combos:\n","            window_labels = '-'.join(window_combo)\n","            print(f\"  > Window Combination: {window_labels}\")\n","\n","            window_features = []\n","            for w in window_combo:\n","                window_features += window_groups[w]\n","\n","            base_features = day_features + window_features + other_features\n","            available_features = [f for f in base_features if f in df.columns]\n","\n","            for target in target_features:\n","                print(f\"    --- Now modeling TARGET VARIABLE: {target} ---\")\n","\n","                for trial_group in range(3):  # 3 groups of trials\n","                    if len(available_features) >= 10:\n","                        sampled_features = random.sample(available_features, 10)\n","                    else:\n","                        sampled_features = available_features\n","\n","                    for trial in range(3):  # 3 trials per group\n","                        params = random.choice(HYPERPARAM_GRID)\n","                        y = df[target]\n","                        valid_idx = y.dropna().index\n","                        df_valid = df.loc[valid_idx].copy()\n","                        df_valid = df_valid.sort_values(by='date')\n","\n","                        # --- Custom Date Split ---\n","                        train_mask = (df_valid['date'] >= '2025-01-01') & (df_valid['date'] <= '2025-03-28')\n","                        test_mask = (df_valid['date'] >= '2025-04-01') & (df_valid['date'] <= '2025-05-30')\n","\n","                        df_train = df_valid.loc[train_mask]\n","                        df_test = df_valid.loc[test_mask]\n","\n","                        if df_train.empty or df_test.empty or len(df_train) < 100 or len(df_test) < 50:\n","                            print(\"âš ï¸ Skipping due to insufficient data in date-based split.\")\n","                            continue\n","\n","                        X_train = df_train[sampled_features]\n","                        y_train = df_train[target]\n","                        X_test = df_test[sampled_features]\n","                        y_test = df_test[target]\n","\n","                        model = create_xgb_model(params)\n","                        start_time = time.time()\n","                        model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n","                        elapsed = time.time() - start_time\n","                        y_pred = model.predict(X_test)\n","\n","                        metrics = {\n","                            'day_group': day_label,\n","                            'window_combo': window_labels,\n","                            'target': target,\n","                            'trial_group': trial_group + 1,\n","                            'trial_in_group': trial + 1,\n","                            'features_used': ', '.join(sampled_features),\n","                            'train_start': df_train['date'].min().strftime('%Y-%m-%d'),\n","                            'train_end': df_train['date'].max().strftime('%Y-%m-%d'),\n","                            'test_start': df_test['date'].min().strftime('%Y-%m-%d'),\n","                            'test_end': df_test['date'].max().strftime('%Y-%m-%d'),\n","                            'max_depth': params['max_depth'],\n","                            'subsample': params['subsample'],\n","                            'colsample_bytree': params['colsample_bytree'],\n","                            'min_child_weight': params['min_child_weight'],\n","                            'gamma': params['gamma'],\n","                            'reg_alpha': params['reg_alpha'],\n","                            'reg_lambda': params['reg_lambda'],\n","                            'colsample_bylevel': params['colsample_bylevel'],\n","                            'colsample_bynode': params['colsample_bynode'],\n","                            'max_delta_step': params['max_delta_step'],\n","                            'accuracy': accuracy_score(y_test, y_pred),\n","                            'precision': precision_score(y_test, y_pred, zero_division=0),\n","                            'recall': recall_score(y_test, y_pred, zero_division=0),\n","                            'f1': f1_score(y_test, y_pred, zero_division=0),\n","                            'time_sec': elapsed\n","                        }\n","                        results.append(metrics)\n","\n","                        print(f\"      âœ… Trial {trial+1}/3 | Acc: {metrics['accuracy']:.3f} | F1: {metrics['f1']:.3f} | Time: {elapsed:.2f}s\")\n","\n","    results_df = pd.DataFrame(results)\n","    results_df.to_csv(OUTPUT_CSV, index=False)\n","    print(f\"\\nðŸŽ¯ All experiments complete! Results saved to {OUTPUT_CSV}\")\n","    return results_df\n","\n","if __name__ == \"__main__\":\n","    final_results = run_modeling()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5abEUEOu5v28"},"outputs":[],"source":["# sliding time series fold cv ( 2month training 1 month testing)\n","\n","\n","import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","import time\n","import os\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# ================== CONFIGURATION ==================\n","DATA_PATH ='/content/drive/MyDrive/Copy of nifty_filtered_features.csv'\n","OUTPUT_CSV = \"nifty_date_based_folds.csv\"\n","\n","INPUT_FEATURES = [\n","   'voi_ratio', 'iv_skew', 'gap', 'curr_day_low_diff', 'gap_direction', 'diff_low_180s',\n","'spot_ret300']\n","TARGETS = [\n","    'iv_target_60', 'iv_target_180', 'iv_target_300',\n","    'realized_vol_target_60', 'realized_vol_target_180', 'realized_vol_target_300',\n","    'spot_dir_target_60', 'spot_dir_target_180', 'spot_dir_target_300'\n","]\n","\n","FIXED_PARAMS = {\n","    'max_depth': 8,\n","    'subsample': 0.8,\n","    'colsample_bytree': 0.8,\n","    'min_child_weight': 6,\n","    'gamma': 0.2,\n","    'reg_alpha': 0.1,\n","    'reg_lambda': 0.3,\n","    'colsample_bylevel': 0.7,\n","    'colsample_bynode': 0.8,\n","    'max_delta_step': 2,\n","    'booster': 'gbtree',\n","    'nthread': 6,\n","    'verbosity': 2,\n","    'validate_parameters': False,\n","    'learning_rate': 0.8,\n","    'n_estimators': 700\n","}\n","\n","# Fold info: (train_start, train_end, test_start, test_end)\n","FOLDS = [\n","    (\"2025-01-01\", \"2025-02-28\", \"2025-03-03\", \"2025-03-28\"),\n","    (\"2025-02-03\", \"2025-03-28\", \"2025-04-01\", \"2025-04-30\"),\n","    (\"2025-03-03\", \"2025-04-30\", \"2025-05-01\", \"2025-05-30\")\n","]\n","\n","def run_modeling():\n","    df = pd.read_csv(DATA_PATH)\n","    df['date'] = pd.to_datetime(df['date'])  # convert date col to datetime\n","    all_results = []\n","\n","    if not os.path.exists(OUTPUT_CSV):\n","        pd.DataFrame(columns=['target', 'fold', 'train_start', 'train_end', 'test_start', 'test_end', 'train_rows', 'test_rows',\n","                              'features_used'] + list(FIXED_PARAMS.keys()) +\n","                             ['accuracy', 'precision', 'recall', 'f1', 'training_time_sec']\n","                    ).to_csv(OUTPUT_CSV, index=False)\n","\n","    global_start = time.time()\n","\n","    for target in TARGETS:\n","        print(f\"\\nðŸ” Processing target: {target}\")\n","        df_target = df.dropna(subset=[target]).copy()\n","\n","        if len(df_target) < 100:\n","            print(f\"âš ï¸ Skipping {target}: Not enough data\")\n","            continue\n","\n","        for fold_num, (train_start, train_end, test_start, test_end) in enumerate(FOLDS, start=1):\n","            # Filter by date range\n","            train_mask = (df_target['date'] >= train_start) & (df_target['date'] <= train_end)\n","            test_mask = (df_target['date'] >= test_start) & (df_target['date'] <= test_end)\n","\n","            df_train = df_target.loc[train_mask]\n","            df_test = df_target.loc[test_mask]\n","\n","            if len(df_train) < 50 or len(df_test) < 50:\n","                print(f\"âš ï¸ Fold K{fold_num} skipped due to insufficient data\")\n","                continue\n","\n","            X_train = df_train[INPUT_FEATURES]\n","            y_train = df_train[target]\n","\n","            X_test = df_test[INPUT_FEATURES]\n","            y_test = df_test[target]\n","\n","            # Train model\n","            start_time = time.time()\n","            model = xgb.XGBClassifier(**FIXED_PARAMS)\n","            model.fit(X_train, y_train)\n","            training_time = time.time() - start_time\n","\n","            # Evaluate\n","            y_pred = model.predict(X_test)\n","            accuracy = accuracy_score(y_test, y_pred)\n","            precision = precision_score(y_test, y_pred, zero_division=0)\n","            recall = recall_score(y_test, y_pred, zero_division=0)\n","            f1 = f1_score(y_test, y_pred, zero_division=0)\n","\n","            result = {\n","                'target': target,\n","                'fold': f\"K{fold_num}\",\n","                'train_start': train_start,\n","                'train_end': train_end,\n","                'test_start': test_start,\n","                'test_end': test_end,\n","                'train_rows': len(y_train),\n","                'test_rows': len(y_test),\n","                'features_used': ', '.join(INPUT_FEATURES),\n","                **FIXED_PARAMS,\n","                'accuracy': accuracy,\n","                'precision': precision,\n","                'recall': recall,\n","                'f1': f1,\n","                'training_time_sec': training_time\n","            }\n","\n","            print(f\"\\nâœ… Fold K{fold_num} for {target}:\")\n","            print(f\"Train: {train_start} â†’ {train_end} ({len(y_train)} rows)\")\n","            print(f\"Test:  {test_start} â†’ {test_end} ({len(y_test)} rows)\")\n","            print(f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n","            print(\"------------\")\n","\n","            pd.DataFrame([result]).to_csv(OUTPUT_CSV, mode='a', header=False, index=False)\n","            all_results.append(result)\n","\n","    total_time = time.time() - global_start\n","    print(f\"\\nðŸŽ¯ All folds processed in {total_time/60:.1f} minutes\")\n","    print(f\"ðŸ“ Results saved to: {OUTPUT_CSV}\")\n","    return pd.DataFrame(all_results)\n","\n","if __name__ == \"__main__\":\n","    final_results = run_modeling()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGPvP9CIXfsG"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import time\n","import warnings\n","from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import VotingClassifier\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","warnings.filterwarnings(\"ignore\")\n","\n","# ================== CONFIG ==================\n","DATA_PATH = '/content/drive/MyDrive/Copy of nifty_filtered_features.csv'\n","TARGET = 'realized_vol_target_300'  # you can loop over multiple targets\n","DATE_COL = 'date'\n","\n","# BEST features (optimized)\n","FEATURES = [\n","   'voi_ratio', 'iv_skew', 'gap_pct', 'curr_day_high_diff', 'curr_day_low_diff',\n","'diff_low_180s', 'diff_high_180s', 'spot_ret300', 'spot_rv180',\n","'iv_ret60', 'iv_rv180', 'iv_stability_180',\n","'fut_vol_chg180', 'fut_ret180', 'spot_stability_180'\n","\n","]\n","\n","# ================== PIPELINE START ==================\n","def load_data():\n","    df = pd.read_csv(DATA_PATH)\n","    df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n","    df = df.dropna(subset=[TARGET])\n","    return df\n","\n","def preprocess(df, train_start, train_end, test_start, test_end):\n","    # Filter by date\n","    train_df = df[(df[DATE_COL] >= train_start) & (df[DATE_COL] <= train_end)].sample(n=1000000, random_state=42)\n","    test_df = df[(df[DATE_COL] >= test_start) & (df[DATE_COL] <= test_end)].sample(n=250000, random_state=42)\n","\n","    # Features and target\n","    X_train = train_df[FEATURES]\n","    y_train = train_df[TARGET]\n","    X_test = test_df[FEATURES]\n","    y_test = test_df[TARGET]\n","\n","    return X_train, y_train, X_test, y_test\n","\n","def build_pipeline():\n","    scaler = StandardScaler()\n","    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n","\n","    xgb_model = XGBClassifier(\n","        n_estimators=1500,\n","        learning_rate=0.05,\n","        max_depth=8,\n","        subsample=0.85,\n","        colsample_bytree=0.8,\n","        reg_alpha=0.05,\n","        reg_lambda=1.0,\n","        gamma=0.2,\n","        use_label_encoder=False,\n","        eval_metric='logloss',\n","        n_jobs=-1\n","    )\n","\n","    lgb_model = LGBMClassifier(\n","        n_estimators=1500,\n","        learning_rate=0.05,\n","        max_depth=8,\n","        subsample=0.85,\n","        colsample_bytree=0.8,\n","        reg_alpha=0.1,\n","        reg_lambda=1.0,\n","        class_weight='balanced',\n","        n_jobs=-1\n","    )\n","\n","    # Combine into soft voting ensemble\n","    ensemble = VotingClassifier(\n","        estimators=[('xgb', xgb_model), ('lgb', lgb_model)],\n","        voting='soft',\n","        n_jobs=-1\n","    )\n","\n","    # Full pipeline\n","    pipeline = Pipeline([\n","        ('scale', scaler),\n","        ('poly', poly),\n","        ('model', ensemble)\n","    ])\n","\n","    return pipeline\n","\n","def evaluate(y_true, y_pred):\n","    acc = accuracy_score(y_true, y_pred)\n","    prec = precision_score(y_true, y_pred, zero_division=0)\n","    rec = recall_score(y_true, y_pred, zero_division=0)\n","    f1 = f1_score(y_true, y_pred, zero_division=0)\n","    return acc, prec, rec, f1\n","\n","def run_final():\n","    df = load_data()\n","    X_train, y_train, X_test, y_test = preprocess(\n","        df, \"2025-01-01\", \"2025-04-30\", \"2025-05-01\", \"2025-05-30\"\n","    )\n","\n","    pipeline = build_pipeline()\n","\n","    print(\"ðŸš€ Training ensemble model...\")\n","    start = time.time()\n","    pipeline.fit(X_train, y_train)\n","    print(f\"âœ… Model trained in {(time.time() - start):.1f} seconds\")\n","\n","    print(\"ðŸ“Š Evaluating...\")\n","    y_pred = pipeline.predict(X_test)\n","    acc, prec, rec, f1 = evaluate(y_test, y_pred)\n","\n","    print(f\"\\nðŸŽ¯ Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n","    return acc, prec, rec, f1\n","\n","# ================== EXECUTE ==================\n","if __name__ == \"__main__\":\n","    run_final()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8274,"status":"ok","timestamp":1751875126272,"user":{"displayName":"kshitiz Garg","userId":"07822256762517209730"},"user_tz":-330},"id":"NlvwJW3K47AA","outputId":"db28a347-a2b8-4998-e31b-9ce24526d7eb"},"outputs":[{"data":{"text/plain":["Index(['date', 'time', 'dte', 'spot_ltp', 'fut_1_ltp', 'fut_1_vol', 'fut_1_oi',\n","       'atm_c_ltp', 'atm_c_iv', 'atm_p_ltp', 'atm_p_iv'],\n","      dtype='object')"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","df=pd.read_csv(\"/content/drive/MyDrive/colab/Copy of nifty_spot_fut_data.csv\")\n","df.columns"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"125rV96f6Fn0XOdREGyjoxwrLyY6u4CbO","authorship_tag":"ABX9TyPMTWSSLSiViQ+kI/7YOhYE"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}